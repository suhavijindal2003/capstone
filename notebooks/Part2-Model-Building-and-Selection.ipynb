{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI ENTERPRISE WORKFLOW CERTIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capstone Project - Part 2. Model Building and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. State the different modelling approaches that you will compare to address the opportunity at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need not necessarily treat this problem as a time series one. We can take the past revenue as a feature and use regression models. To ensure that do violate any of the assumptions required for a Linear Regression, we shall primarily focus on tree based regression methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis will work as follows:\n",
    "\n",
    "**Preprocessing:**\n",
    "* Feature Engineer\n",
    "* Train_Test_Split\n",
    "* Standard Scaling\n",
    "\n",
    "\n",
    "**Model Selection:**\n",
    "\n",
    "For each of the top 10 countries train a separate model:\n",
    "* Decision Tree Regression\n",
    "* Random Forest Regression\n",
    "* Gradient Boost Regression\n",
    "* Ada Boost Regression\n",
    "* XGBoost Regression\n",
    "\n",
    "Tune parameters on each of these models, and the select which is the best for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting logger.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile logger.py\n",
    "\n",
    "import time\n",
    "import os \n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import uuid\n",
    "import joblib\n",
    "from datetime import date\n",
    "\n",
    "#Load from \n",
    "from project_setup import PROJECT_DATA_DIR, LOG_DIR\n",
    "\n",
    "\n",
    "def _update_train_log(tag, best_model, model_index, mse_score, data_shape, runtime, MODEL_VERSION,\\\n",
    "                     MODEL_VERSION_NOTE, test):\n",
    "    \"\"\"\n",
    "    update train log file\n",
    "    \"\"\"\n",
    "\n",
    "    ## Ensure correct directory exists\n",
    "    if not os.path.exists(LOG_DIR):\n",
    "        os.makedirs(LOG_DIR)\n",
    "\n",
    "    ## name the logfile using something that cycles with date (day, month, year)    \n",
    "    today = date.today()\n",
    "    if test:\n",
    "        logfile = os.path.join(LOG_DIR,\"train-test.log\")\n",
    "    else:\n",
    "        logfile = os.path.join(LOG_DIR,\"train-{}-{}.log\".format(today.year, today.month))\n",
    "        \n",
    "    ## write the data to a csv file    \n",
    "    header = ['unique_id','timestamp', 'country','algorithm','mse_score','data_shape','runtime','Model_Version',\n",
    "              'Model_Version_Note', 'test']\n",
    "              \n",
    "    write_header = False\n",
    "    if not os.path.exists(logfile):\n",
    "        write_header = True\n",
    "    with open(logfile,'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        if write_header:\n",
    "            writer.writerow(header)\n",
    "\n",
    "        to_write = map(str,[uuid.uuid4(),time.time(),tag ,best_model, mse_score, data_shape, runtime,\n",
    "                            MODEL_VERSION, MODEL_VERSION_NOTE, test])\n",
    "        writer.writerow(to_write)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def _update_predict_log(tag, y_pred, target_date, MODEL_VERSION, MODEL_VERSION_NOTE):\n",
    "    \"\"\"\n",
    "    Update predict log file\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Ensure correct directory exists\n",
    "    if not os.path.exists(LOG_DIR):\n",
    "        os.makedirs(LOG_DIR)\n",
    "    \n",
    "    ## Name the logfile using something that cycles with date (day, month, year)\n",
    "    today = date.today()\n",
    "    logfile = 'predict-{}-{}.log'.format(today.year, today.month)\n",
    "\n",
    "    \n",
    "    ## Write the log to a csv file\n",
    "    logpath = os.path.join(LOG_DIR, logfile)\n",
    "    \n",
    "    \n",
    "    header = ['unique_id', 'timestamp', 'y_pred','target_date' ,'Model_Version', 'Model_Version_Note']\n",
    "    write_header = False\n",
    "    if not os.path.exists(logpath):\n",
    "        write_header = True\n",
    "    with open(logpath,'a') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|')\n",
    "        if write_header:\n",
    "            writer.writerow(header)\n",
    "\n",
    "        to_write = map(str,[uuid.uuid4(),time.time(), tag,  y_pred,  target_date, \n",
    "                            MODEL_VERSION, MODEL_VERSION_NOTE])\n",
    "        writer.writerow(to_write)\n",
    "\n",
    "        \n",
    "def log_load(env,year,month,verbose=True):\n",
    "    \"\"\"\n",
    "    load requested log file\n",
    "    \"\"\"\n",
    "    logfile = \"{}-{}-{}.log\".format(env,year,month)\n",
    "    \n",
    "    if verbose:\n",
    "        print(logfile)\n",
    "    return logfile\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_modelling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_modelling.py\n",
    "\n",
    "\n",
    "#Standard Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import re\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Modelling\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "\n",
    "#Load from \n",
    "from project_setup import PROJECT_DATA_DIR, DATA_DIR, TS_DIR, LOG_DIR, MODEL_DIR\n",
    "\n",
    "from data_ingestion import load_ts, engineer_features\n",
    "from logger import _update_predict_log, _update_train_log\n",
    "\n",
    "MODEL_VERSION = 0.1\n",
    "MODEL_VERSION_NOTE = '-'\n",
    "\n",
    "\n",
    "def _model_train(dataset,tag,test = False):\n",
    "    \"\"\"\n",
    "    Train models and select the best one out of DecisionTreeRegression,  GradientBoostingRegression, AdaBoostRegression\n",
    "    and XGBoostRegressor. Feed the model the timeseries_datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## start timer for runtime\n",
    "    time_start = time.time()\n",
    "    \n",
    "    dataset = engineer_features(dataset, training = True)\n",
    "    \n",
    "    X = dataset.drop(['target','dates'], axis = 1)\n",
    "    y = dataset.target\n",
    "    \n",
    "    #Train_Test_Split Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33, random_state = 0)\n",
    "    \n",
    "    \n",
    "    ##Train Models\n",
    "    \n",
    "    GridSearchParameters = {'criterion': ['mse', 'mae', 'friedman_mse'],\n",
    "                            'max_depth': [None, 10,20,50],\n",
    "                            'max_features': ['auto', 'sqrt', 'log2']}, \\\n",
    "    {'criterion': ['mse', 'mae'],\n",
    "     'max_features' : ['auto', 'sqrt'] }, \\\n",
    "    {'loss' : ['ls', 'lad', 'huber', 'quantile'],\n",
    "     'learning_rate' : [0.1,0.01,0.001]}, \\\n",
    "    {'loss' : ['linear', 'square',],\n",
    "     'learning_rate' : [0.05, 0.1, 0.01]}, \\\n",
    "    {'learning_rate': [0.05, 0.1, 0.01],\n",
    "     'max_depth': [1, 5, 50],\n",
    "     'n_estimators': [100, 1000, 500]\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'DTR_P' : GridSearchParameters[0],\n",
    "        'RFR_P' : GridSearchParameters[1],\n",
    "        'GBR_P' : GridSearchParameters[2],\n",
    "        'ADA_P' : GridSearchParameters[3],\n",
    "        'XGB_P' : GridSearchParameters[4],\n",
    "    }\n",
    "    \n",
    "    regressor_dict = {\n",
    "        'DTR' : DecisionTreeRegressor(random_state = 42),\n",
    "        'RFR' : RandomForestRegressor(random_state = 42),\n",
    "        'GBR' : GradientBoostingRegressor(random_state = 42),\n",
    "        'ADA' : AdaBoostRegressor(random_state = 42),\n",
    "        'XGB' : xgb.XGBRegressor(seed = 42)\n",
    "\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    for model_name in regressor_dict:\n",
    "        \n",
    "        pipe = Pipeline(steps = [('scaler', StandardScaler()),\n",
    "                                ('regressor', regressor_dict[model_name])])\n",
    "        grid = GridSearchCV(regressor_dict[model_name],\n",
    "                           param_grid = params[model_name + '_P'], cv = 5)\n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        models[model_name] = grid\n",
    "        \n",
    "     \n",
    "    model_scores = []\n",
    "    \n",
    "    #Test which model is optimal.\n",
    "    for model in models:\n",
    "        y_pred = models[model].predict(X_test)\n",
    "        rmse = np.sqrt(mse(y_pred, y_test))\n",
    "        model_scores.append(rmse)\n",
    "    \n",
    "    model_index = np.argmin(model_scores)\n",
    "    model_score = min(model_scores)\n",
    "    model_name = list(models.keys())[model_index]\n",
    "    best_model =  list(models.values())[model_index]\n",
    "    \n",
    "    print(f'The best model for {tag} is {model_name}.')\n",
    "   \n",
    "    \n",
    "    #Retrain on best model.\n",
    "    best_model.fit(X,y)\n",
    "    \n",
    "    #Save model.\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.mkdir(MODEL_DIR)\n",
    "    \n",
    "    if test:\n",
    "        saved_model = os.path.join(MODEL_DIR, f'test-{tag}-{model_name}.joblib')\n",
    "    else:\n",
    "        saved_model = os.path.join(MODEL_DIR, f'sl-{tag}-{model_name}.joblib')\n",
    "        \n",
    "    \n",
    "    joblib.dump(best_model,saved_model)\n",
    "    \n",
    "    m, s = divmod(time.time()-time_start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    runtime = \"%03d:%02d:%02d\"%(h, m, s)\n",
    "    \n",
    "    \n",
    "    # Update Train Log.\n",
    "    _update_train_log(tag, best_model, model_index, model_score, dataset.shape, runtime, MODEL_VERSION,\n",
    "                     MODEL_VERSION_NOTE, test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def model_train(ts_dir = TS_DIR, test = False):\n",
    "    \"\"\"\n",
    "    Train the models for each of the top ten countries (+ all).\n",
    "    \"\"\"\n",
    "    #Check Directories\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.mkdir(MODEL_DIR)\n",
    "    \n",
    "    #Load ts files\n",
    "    ts = load_ts(ts_dir)\n",
    "    \n",
    "    for country,df in ts.items():\n",
    "        print(f'...training model for {country}')\n",
    "        _model_train(df, country, test = test)\n",
    "\n",
    "        \n",
    "def model_load(prefix = 'sl',ts_dir = TS_DIR, model_dir = MODEL_DIR):\n",
    "    \"\"\"\n",
    "    Function to load in Train Models\n",
    "    \"\"\"\n",
    "    \n",
    "    model_list = [file for file in os.listdir(model_dir) if file[0:len(prefix)] == prefix]\n",
    "    \n",
    "    if len(model_list) == 0:\n",
    "        raise Exception(f'No models found with prefix: {prefix}. Did you train them?')\n",
    "        \n",
    "    models = {}\n",
    "    print('...Loading models')\n",
    "    for model in model_list:\n",
    "        models[re.split('-',model)[1]] = joblib.load(os.path.join(model_dir,model))\n",
    "        \n",
    "\n",
    "        \n",
    "    return models\n",
    "\n",
    "    \n",
    "def model_predict(year, month, day, country = 'all'):\n",
    "    \"\"\"\n",
    "    Make predictions based on a country.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Timer\n",
    "    time_start = time.time()\n",
    "    \n",
    "    ## Load all data\n",
    "    ts = load_ts()\n",
    "    eng_datasets = {country: engineer_features(ts[country], training = False) for country in ts.keys()}\n",
    "    \n",
    "    \n",
    "    ## Load all models\n",
    "    models = model_load()\n",
    "    \n",
    "    \n",
    "    ## check if model for country \n",
    "    if country not in models.keys():\n",
    "        raise Exception(f'ERROR: (model_predict) for country {country} is unavailable.')\n",
    "        \n",
    "        \n",
    "    ## Check if dataset is available\n",
    "    if country not in eng_datasets.keys():\n",
    "        raise Exception(f'ERROR: (dataset) for country {country} is unavailable.')\n",
    "    \n",
    " \n",
    "    ## Load data and model for country\n",
    "    model = models[country]\n",
    "    eng_dataset = eng_datasets[country]\n",
    "    \n",
    "    \n",
    "    ## Check date\n",
    "    target_date = f'{year}-{str(month).zfill(2)}-{str(day).zfill(2)}'\n",
    "    print(target_date)\n",
    "    \n",
    "    ## Data to predict on:\n",
    "    X_pred = eng_dataset[eng_dataset['dates'] == target_date].drop(['target','dates'], axis =1 )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Prediction\n",
    "    y_pred = model.predict(X_pred)\n",
    "    \n",
    "    _update_predict_log(tag = country, y_pred = y_pred,  target_date = target_date,\\\n",
    "                        MODEL_VERSION = MODEL_VERSION, MODEL_VERSION_NOTE = MODEL_VERSION_NOTE)\n",
    "    \n",
    "    return(y_pred)       \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"\n",
    "    basic test procedure for model.py\n",
    "    \"\"\"\n",
    "    \n",
    "    run_start = time.time()\n",
    "\n",
    "    ## train the models if directory not exists\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        print(\"...Training Models\")\n",
    "        model_train(TS_DIR)\n",
    "\n",
    "    ## load the model\n",
    "    print(\"...Loading Models\")\n",
    "    models = model_load()\n",
    "    print(\"...models loaded: \",\",\".join(models.keys()))\n",
    "\n",
    "    ## test predict\n",
    "    country='all'\n",
    "    year='2018'\n",
    "    month='1'\n",
    "    day='13'\n",
    "    result = model_predict(year = year, month = month, day = day, country = country)\n",
    "    \n",
    "    m, s = divmod(time.time()-run_start,60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(\"...running time:\", \"%d:%02d:%02d\"%(h, m, s))\n",
    "    \n",
    "    \n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Training Models\n",
      "Ingesting timeseries data from files.\n",
      "...training model for EIRE\n",
      "The best model for EIRE is RFR.\n",
      "...training model for Hong Kong\n",
      "The best model for Hong Kong is RFR.\n",
      "...training model for Germany\n",
      "The best model for Germany is RFR.\n",
      "...training model for Netherlands\n",
      "The best model for Netherlands is XGB.\n",
      "...training model for France\n",
      "The best model for France is RFR.\n",
      "...training model for Singapore\n",
      "The best model for Singapore is ADA.\n",
      "...training model for Spain\n",
      "The best model for Spain is GBR.\n",
      "...training model for all\n",
      "The best model for all is GBR.\n",
      "...training model for United Kingdom\n",
      "The best model for United Kingdom is XGB.\n",
      "...training model for Norway\n",
      "The best model for Norway is DTR.\n",
      "...training model for Portugal\n",
      "The best model for Portugal is XGB.\n",
      "...Loading Models\n",
      "...Loading models\n",
      "...models loaded:  all,France,United Kingdom,Hong Kong,Singapore,EIRE,Netherlands,Germany,Portugal,Spain,Norway\n",
      "Ingesting timeseries data from files.\n",
      "...Loading models\n",
      "2018-01-13\n",
      "...running time: 0:08:14\n",
      "[162245.57369781]\n"
     ]
    }
   ],
   "source": [
    "%run data_modelling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
