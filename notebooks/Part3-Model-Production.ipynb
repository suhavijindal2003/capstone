{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI ENTERPRISE WORKFLOW CERTIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capstone Project - Part 3.  Model Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline:\n",
    "\n",
    "1. Build a draft version of an API with train, predict, and logfile endpoints.\n",
    "2. Using Docker, bundle your API, model, and unit tests.\n",
    "3. Using test-driven development iterate on your API in a way that anticipates scale, load, and drift.\n",
    "4. Create a post-production analysis script that investigates the relationship between model performance and the business metric.\n",
    "5. Articulate your summarized findings in a final report.\n",
    "\n",
    "\n",
    "\n",
    "At a high level you are being asked to:\n",
    "1. Ready for your model for deployment.\n",
    "2. Query your API with new data and test your monitoring tools.\n",
    "3. Compare results to the gold standard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an API with FLASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import argparse\n",
    "from flask import Flask, jsonify, request\n",
    "from flask import render_template\n",
    "import joblib\n",
    "import socket\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "## import model specific functions and variables\n",
    "from project_setup import *\n",
    "from data_modelling import *\n",
    "from logger import *\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "# Predict \n",
    "@app.route('/predict', methods=['GET','POST'])\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    basic predict function for the API\n",
    "    \"\"\"\n",
    "    \n",
    "    ## input checking\n",
    "    if not request.json:\n",
    "        print(\"ERROR: API (predict): did not receive request data\")\n",
    "        return jsonify([])\n",
    "    \n",
    "    if 'country' not in request.json:\n",
    "        print(\"ERROR API (predict): received request, but no 'country' found within\")\n",
    "        return jsonify(False)\n",
    "        \n",
    "    if 'year' not in request.json:\n",
    "        print(\"ERROR API (predict): received request, but no 'year' found within\")\n",
    "        return jsonify(False)\n",
    "        \n",
    "    if 'month' not in request.json:\n",
    "        print(\"ERROR API (predict): received request, but no 'month' found within\")\n",
    "        return jsonify(False)\n",
    "        \n",
    "    if 'day' not in request.json:\n",
    "        print(\"ERROR API (predict): received request, but no 'day' found within\")\n",
    "        return jsonify(False)\n",
    "\n",
    "        \n",
    "    ## predict\n",
    "    _result = result = model_predict(year=request.json['year'],\n",
    "                                     month=request.json['month'],\n",
    "                                     day=request.json['day'],\n",
    "                                     country=request.json['country'],\n",
    "                                    )\n",
    "    \n",
    "    ## convert numpy objects so ensure they are serializable\n",
    "    result = result.tolist()\n",
    "    \n",
    "\n",
    "\n",
    "    return(jsonify(result))\n",
    "\n",
    "# Train \n",
    "@app.route('/train', methods=['GET','POST'])\n",
    "def train():\n",
    "    \"\"\"\n",
    "    basic train function for the API\n",
    "\n",
    "    the 'dev' give you the ability to toggle between a DEV version and a PROD verion of training\n",
    "    \"\"\"\n",
    "\n",
    "    if not request.json:\n",
    "        print(\"ERROR: API (train): did not receive request data\")\n",
    "        return jsonify(False)\n",
    "\n",
    "    if 'test' not in request.json:\n",
    "        print(\"ERROR API (train): received request, but no 'test' found within\")\n",
    "        return jsonify(False)\n",
    "\n",
    "    print(\"... training model\")\n",
    "    model = model_train()\n",
    "    print(\"... training complete\")\n",
    "\n",
    "    return(jsonify(True))\n",
    "\n",
    "# Log\n",
    "@app.route('/logging', methods=['GET','POST'])\n",
    "def load_logs():\n",
    "    \"\"\"\n",
    "    basic logging function for the API\n",
    "    \"\"\"\n",
    "\n",
    "    if not request.json:\n",
    "        print(\"ERROR: API (train): did not receive request data\")\n",
    "        return jsonify(False)\n",
    "\n",
    "    if 'env' not in request.json:\n",
    "        print(\"ERROR API (log): received request, but no 'env' found within\")\n",
    "        return jsonify(False)\n",
    "        \n",
    "        \n",
    "    if 'month' not in request.json:\n",
    "        print(\"ERROR API (log): received request, but no 'month' found within\")\n",
    "        return jsonify(False)\n",
    "        \n",
    "    if 'year' not in request.json:\n",
    "        print(\"ERROR API (log): received request, but no 'year' found within\")\n",
    "        return jsonify(False)\n",
    "    \n",
    "    print(\"... fetching logfile\")\n",
    "    logfile = log_load(env=request.json['env'],\n",
    "                       year=request.json['year'],\n",
    "                       month=request.json['month'])\n",
    "    \n",
    "    result = {}\n",
    "    result[\"logfile\"]=logfile\n",
    "    return(jsonify(result))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ## parse arguments for debug mode\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"-d\", \"--debug\", action=\"store_true\", help=\"debug flask\")\n",
    "    args = vars(ap.parse_args())\n",
    "\n",
    "    if args[\"debug\"]:\n",
    "        app.run(debug=True, port=8080)\n",
    "    else:\n",
    "        app.run(host='0.0.0.0', threaded=True ,port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the app "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the FLASK App - go into the directory where app.py is stored and run with:\n",
    "```\n",
    "python3 app.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## API predict\n",
    "import requests\n",
    "from ast import literal_eval\n",
    "\n",
    "query = {\"year\":\"2018\",\"month\":\"2\",\"day\":\"11\",\"country\":\"all\"}\n",
    "port = 8080\n",
    "r = requests.post(f'http://localhost:{port}/predict',json=query)\n",
    "response = literal_eval(r.text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## API train\n",
    "query = {\"ts_dir\":\"TS_DIR\", \"test\":\"False\"}\n",
    "port = 8080\n",
    "r = requests.post('http://localhost:{}/train'.format(port),json=query)\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## API Log\n",
    "query = {\"env\":\"train\",\"year\":\"2021\",\"month\":\"04\"}\n",
    "port = 8080\n",
    "r = requests.post('http://localhost:{}/logging'.format(port),json=query)\n",
    "response = literal_eval(r.text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Load from \n",
    "from project_setup import PROJECT_DATA_DIR, UNITTEST_DIR\n",
    "\n",
    "# create path if needed\n",
    "if not os.path.exists(UNITTEST_DIR):\n",
    "    os.mkdir(os.path.join(UNITTEST_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting runtime/unittests/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile runtime/unittests/__init__.py\n",
    "\n",
    "import unittest\n",
    "import getopt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## parse inputs\n",
    "try:\n",
    "    optlist, args = getopt.getopt(sys.argv[1:],'v')\n",
    "except getopt.GetoptError:\n",
    "    print(getopt.GetoptError)\n",
    "    print(sys.argv[0] + \"-v\")\n",
    "    print(\"... the verbose flag (-v) may be used\")\n",
    "    sys.exit()\n",
    "\n",
    "VERBOSE = False\n",
    "RUNALL = False\n",
    "\n",
    "sys.path.append(os.path.realpath(os.path.dirname(__file__)))\n",
    "\n",
    "for o, a in optlist:\n",
    "    if o == '-v':\n",
    "        VERBOSE = True\n",
    "\n",
    "## api tests\n",
    "from ApiTests import *\n",
    "ApiTestSuite = unittest.TestLoader().loadTestsFromTestCase(ApiTest)\n",
    "\n",
    "## model tests\n",
    "from ModelTests import *\n",
    "ModelTestSuite = unittest.TestLoader().loadTestsFromTestCase(ModelTest)\n",
    "\n",
    "## logger tests\n",
    "from LoggerTests import *\n",
    "LoggerTestSuite = unittest.TestLoader().loadTestsFromTestCase(LoggerTest)\n",
    "\n",
    "MainSuite = unittest.TestSuite([LoggerTestSuite,ModelTestSuite,ApiTestSuite])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting runtime/unittests/ModelTests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile runtime/unittests/ModelTests.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "model tests\n",
    "\"\"\"\n",
    "\n",
    "import unittest\n",
    "from project_setup import PROJECT_DATA_DIR, UNITTEST_DIR, MODEL_DIR, TEST, TS_DIR\n",
    "\n",
    "from data_modelling import *\n",
    "\n",
    "class ModelTest(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    test the essential functionality\n",
    "    \"\"\"\n",
    "    \n",
    "    def test_01_train(self):\n",
    "        \"\"\"\n",
    "        test the train functionality\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Test: Model-Train\")\n",
    "        \n",
    "        ## train the model\n",
    "        model_train(TS_DIR, TEST)\n",
    "        \n",
    "        prefix = 'test' if TEST else 'sl'\n",
    "        models = [f for f in os.listdir(MODEL_DIR) if re.search(prefix,f)]\n",
    "        self.assertEqual(len(models),11)\n",
    "        \n",
    "    def test_02_load(self):\n",
    "        \"\"\"\n",
    "        test the train functionality\n",
    "        \"\"\"\n",
    "        print(\"Test: Model-Load\")\n",
    "       \n",
    "        ## load the model\n",
    "        models = model_load()\n",
    "        \n",
    "        for tag, model in models.items():\n",
    "            self.assertTrue(\"predict\" in dir(model))\n",
    "            self.assertTrue(\"fit\" in dir(model))\n",
    "        \n",
    "        \n",
    "    def test_03_predict(self):\n",
    "        \"\"\"\n",
    "        test the predict function input\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Test: Model-Predict-Input\")\n",
    "    \n",
    "        ## query inputs\n",
    "        query = [\"2018\", \"1\", \"5\", \"all\"]\n",
    "        \n",
    "        ## load model first\n",
    "        y_pred = model_predict(year=query[0], month=query[1], day=query[2], country=query[3])\n",
    "        self.assertTrue(y_pred.dtype==np.float64)\n",
    "        \n",
    "               \n",
    "    def test_04_predict(self):\n",
    "        \"\"\"\n",
    "        test the predict function accuracy\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Test: Model-Predict-Accuracy\")\n",
    "   \n",
    "         ## example predict\n",
    "        example_queries = [[\"2018\", \"11\", \"02\", \"all\"],\n",
    "                           [\"2019\", \"01\", \"01\", \"EIRE\"],\n",
    "                           [\"2018\", \"03\", \"05\", \"all\"]]\n",
    "\n",
    "        for query in example_queries:\n",
    "            y_pred = model_predict(year=query[0], month=query[1], day=query[2], country=query[3])\n",
    "            self.assertTrue(y_pred.dtype==np.float64)\n",
    "        \n",
    "## run the tests\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Model-Train\n",
      "Ingesting timeseries data from files.\n",
      "...training model for EIRE\n",
      "The best model for EIRE is RFR.\n",
      "...training model for Hong Kong\n",
      "The best model for Hong Kong is RFR.\n",
      "...training model for Germany\n",
      "The best model for Germany is RFR.\n",
      "...training model for Netherlands\n",
      "The best model for Netherlands is XGB.\n",
      "...training model for France\n",
      "The best model for France is RFR.\n",
      "...training model for Singapore\n",
      "The best model for Singapore is ADA.\n",
      "...training model for Spain\n",
      "The best model for Spain is GBR.\n",
      "...training model for all\n",
      "The best model for all is GBR.\n",
      "...training model for United Kingdom\n",
      "The best model for United Kingdom is XGB.\n",
      "...training model for Norway\n",
      "The best model for Norway is DTR.\n",
      "...training model for Portugal\n",
      "The best model for Portugal is XGB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Model-Load\n",
      "...Loading models\n",
      "Test: Model-Predict-Input\n",
      "Ingesting timeseries data from files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Loading models\n",
      "2018-01-05\n",
      "Test: Model-Predict-Accuracy\n",
      "Ingesting timeseries data from files.\n",
      "...Loading models\n",
      "2018-11-02\n",
      "Ingesting timeseries data from files.\n",
      "...Loading models\n",
      "2019-01-01\n",
      "Ingesting timeseries data from files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Loading models\n",
      "2018-03-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 489.467s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run runtime/unittests/ModelTests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting runtime/unittests/LoggerTests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile runtime/unittests/LoggerTests.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "logger tests\n",
    "\"\"\"\n",
    "\n",
    "import unittest\n",
    "## import model specific functions and variables\n",
    "from logger import *\n",
    "\n",
    "class LoggerTest(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    test the essential log functionality\n",
    "    \"\"\"\n",
    "        \n",
    "    def test_01_train(self):\n",
    "        \"\"\"\n",
    "        test the train functionality\n",
    "        \"\"\"\n",
    "\n",
    "        ## train logfile\n",
    "        today = date.today()\n",
    "        logfile = \"{}-{}-{}.log\".format(\"train\",today.year,today.month)\n",
    "        log_path = os.path.join(LOG_DIR, logfile)\n",
    "        \n",
    "        self.assertTrue(os.path.exists(log_path))\n",
    "\n",
    "\n",
    "\n",
    "    def test_02_predict(self):\n",
    "        \"\"\"\n",
    "        test the predict functionality\n",
    "        \"\"\"\n",
    "        \n",
    "        ## train logfile\n",
    "        today = date.today()\n",
    "        logfile = \"{}-{}-{}.log\".format(\"predict\",today.year,today.month)\n",
    "        log_path = os.path.join(LOG_DIR, logfile)\n",
    "        \n",
    "        self.assertTrue(os.path.exists(log_path))\n",
    "\n",
    "        \n",
    "        \n",
    "    def test_03_load(self):\n",
    "        \"\"\"\n",
    "        test the load functionality\n",
    "        \"\"\"\n",
    "\n",
    "        ## load model first\n",
    "        logfile = log_load(env = 'train',year=2021,month=4, verbose=False)\n",
    "        logpath = os.path.join(LOG_DIR, logfile)\n",
    "        with open(logpath, \"r\") as log:\n",
    "            text = log.read()\n",
    "        self.assertTrue(len(text.split(\"\\n\"))>2)        \n",
    "        \n",
    "        \n",
    "### Run the tests\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run runtime/unittests/LoggerTests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting runtime/unittests/ApiTests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile runtime/unittests/ApiTests.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "api tests\n",
    "\n",
    "these tests use the requests package however similar requests can be made with curl\n",
    "\n",
    "e.g.\n",
    "data = '{\"key\":\"value\"}'\n",
    "curl -X POST -H \"Content-Type: application/json\" -d \"%s\" http://localhost:8080/predict'%(data)\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import unittest\n",
    "import requests\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "port = 8080\n",
    "\n",
    "try:\n",
    "    requests.post('http://localhost:{}/predict'.format(port))\n",
    "    server_available = True\n",
    "except:\n",
    "    server_available = False\n",
    "    \n",
    "## test class for the main window function\n",
    "class ApiTest(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    test the essential functionality\n",
    "    \"\"\"\n",
    "    \n",
    "    @unittest.skipUnless(server_available,\"local server is not running\")\n",
    "    def test_predict(self):\n",
    "        \"\"\"\n",
    "        test the predict functionality\n",
    "        \"\"\"\n",
    "\n",
    "        query = {\"year\":\"2019\",\"month\":\"2\",\"day\":\"1\",\"country\":\"all\"}\n",
    "        r = requests.post('http://localhost:{}/predict'.format(port),json=query)\n",
    "        response = literal_eval(r.text)\n",
    "        self.assertTrue(isinstance(response[0],float))\n",
    "\n",
    "    @unittest.skipUnless(server_available,\"local server is not running\")\n",
    "    def test_train(self):\n",
    "        \"\"\"\n",
    "        test the train functionality\n",
    "        \"\"\"\n",
    "      \n",
    "        query = {\"ts_dir\":\"TS_DIR\", \"test\": \"False\" }\n",
    "        r = requests.post('http://localhost:{}/train'.format(port),json=query)\n",
    "        train_complete = re.sub(\"\\W+\",\"\",r.text)\n",
    "        self.assertEqual(train_complete,'true')   \n",
    "        \n",
    "    def test_logging(self):\n",
    "        \"\"\"\n",
    "        test the logging functionality\n",
    "        \"\"\"\n",
    "        \n",
    "        query = {\"env\":\"train\",\"year\":\"2021\",\"month\":\"04\"}\n",
    "        r = requests.post('http://localhost:{}/logging'.format(port),json=query)\n",
    "        response = literal_eval(r.text)\n",
    "        self.assertEqual(response.get(\"logfile\"),'train-2021-4.log')\n",
    "    \n",
    "### Run the tests\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run runtime/unittests/ApiTests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile runtime/run-tests.py\n",
    "\n",
    "\n",
    "import sys\n",
    "import unittest\n",
    "\n",
    "from unittests import *\n",
    "unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run runtime/run-tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Docker file with all necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the docker environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Load from \n",
    "from project_setup import DOCKER_DIR\n",
    "\n",
    "# create path if needed\n",
    "if not os.path.exists(DOCKER_DIR):\n",
    "    os.mkdir(os.path.join(DOCKER_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writer Requirements file with necessary libs for the container runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile runtime/dockerenv/requirements.txt\n",
    "\n",
    "pandas\n",
    "numpy\n",
    "scikit-learn\n",
    "matplotlib\n",
    "xgboost\n",
    "IPython\n",
    "seaborn\n",
    "flask\n",
    "jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile runtime/dockerenv/Dockerfile\n",
    "\n",
    "# Use an official Python runtime as a parent image\n",
    "FROM python:3.7.5-stretch\n",
    "\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "python3-dev \\\n",
    "build-essential    \n",
    "        \n",
    "# Set the working directory to /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the directory contents into the container at /app\n",
    "ADD ../../. /app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 80 available to the world outside this container\n",
    "EXPOSE 80\n",
    "\n",
    "# Define environment variable\n",
    "ENV NAME World\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
